{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbg8PyFoL2iG",
        "outputId": "e7b51451-fe2b-4fbf-9db4-9b5ca1f08dd0"
      },
      "outputs": [],
      "source": [
        "#!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "#!pip3 install matplotlib\n",
        "#!pip3 install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "RZxh7jS2PuNv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here you must specify the cause_id you want to train on\n",
        "\"\"\"\n",
        "cause_id columns meaning:\n",
        "  \"508\",\"Chronic respiratory diseases\"\n",
        "  \"509\",\"Chronic obstructive pulmonary disease\"\n",
        "  \"510\",\"Pneumoconiosis\"\n",
        "  \"511\",\"Silicosis\"\n",
        "  \"512\",\"Asbestosis\"\n",
        "  \"513\",\"Coal workers pneumoconiosis\"\n",
        "  \"514\",\"Other pneumoconiosis\"\n",
        "  \"515\",\"Asthma\"\n",
        "  \"516\",\"Interstitial lung disease and pulmonary sarcoidosis\"\n",
        "  \"520\",\"Other chronic respiratory diseases\"\n",
        "  \"-1\",\"All causes\"\n",
        "\"\"\"\n",
        "cause_id = 508"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "YQU8wUGfQSnr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device is cpu\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42) # Setting the seed\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "if torch.cuda.is_available(): \n",
        "  torch.cuda.manual_seed(42)\n",
        "  torch.cuda.manual_seed_all(42)\n",
        "print(\"Current device is {}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearClassifier(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "vYudliXoRoOI"
      },
      "outputs": [],
      "source": [
        "class AirDataSet(data.Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    file_path = 'training_data/cause_id_{}.csv'.format(cause_id)\n",
        "    if (cause_id == -1):\n",
        "      file_path = 'training_data/all_data.csv'\n",
        "    training_df = pd.read_csv(file_path,\n",
        "      dtype={\n",
        "        'parameter_85101': 'float32',\n",
        "        'parameter_88101': 'float32',\n",
        "        'parameter_44201': 'float32',\n",
        "        'parameter_42602': 'float32',\n",
        "        'parameter_42401': 'float32',\n",
        "        'parameter_42101': 'float32',\n",
        "        'mortality_rate': 'float32',\n",
        "        'cause_id': 'float32',\n",
        "      })\n",
        "    training_df = training_df.drop(columns=['fips', 'year'])\n",
        "    if (cause_id != -1):\n",
        "      training_df = training_df.drop(columns=['cause_id'])\n",
        "    \n",
        "    print(training_df.dtypes)\n",
        "\n",
        "\n",
        "    # Use only the first 1000 rows for training\n",
        "    # training_df = training_df[:1000]\n",
        "\n",
        "    # For each row we have the following columns corresponding to features:\n",
        "    # -parameter_85101\n",
        "    # -parameter_88101\n",
        "    # -parameter_44201\n",
        "    # -parameter_42602\n",
        "    # -parameter_42401\n",
        "    # -parameter_42101\n",
        "    # -cause_id (if cause_id != -1)\n",
        "\n",
        "    # And the following columns corresponding to labels:\n",
        "    # -rate\n",
        "\n",
        "    # We want to predict the rate based on the parameters\n",
        "    self.data = torch.from_numpy(training_df.drop(columns=['mortality_rate']).to_numpy())\n",
        "    self.label = torch.from_numpy(training_df[['mortality_rate']].to_numpy())\n",
        "\n",
        "    # Normalize the data based on mean and variance\n",
        "    print(\"Data mean: {}, Data std: {}\".format(self.data.mean(dim=0), self.data.std(dim=0)))\n",
        "\n",
        "    self.data = (self.data - self.data.mean(dim=0)) / self.data.std(dim=0)\n",
        "\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
        "    return self.data.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Return the idx-th data point of the dataset\n",
        "    # If we have multiple things to return (data point and label), we can return them as tuple\n",
        "    data_point = self.data[idx]\n",
        "    data_label = self.label[idx]\n",
        "    return data_point, data_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "parameter_85101    float32\n",
            "parameter_88101    float32\n",
            "parameter_44201    float32\n",
            "parameter_42602    float32\n",
            "parameter_42401    float32\n",
            "parameter_42101    float32\n",
            "mortality_rate     float32\n",
            "dtype: object\n",
            "Data mean: tensor([18.1043,  8.8269,  0.0471, 14.1742,  5.2491,  0.5011]), Data std: tensor([8.4844e+00, 3.1032e+00, 8.0862e-03, 1.0613e+01, 1.0969e+01, 4.7684e-01])\n"
          ]
        }
      ],
      "source": [
        "model = LinearClassifier(7, 1)\n",
        "if (cause_id != -1):\n",
        "  model = LinearClassifier(6, 1)\n",
        "\n",
        "dataset = AirDataSet()\n",
        "train_data_loader = data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearClassifier(\n",
              "  (linear): Linear(in_features=6, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 loss: 436.49957275390625\n",
            "Epoch 1 loss: 105.990478515625\n",
            "Epoch 2 loss: 211.677001953125\n",
            "Epoch 3 loss: 259.3853454589844\n",
            "Epoch 4 loss: 165.5889892578125\n",
            "Epoch 5 loss: 646.2418212890625\n",
            "Epoch 6 loss: 93.45532989501953\n",
            "Epoch 7 loss: 171.14505004882812\n",
            "Epoch 8 loss: 60.18762969970703\n",
            "Epoch 9 loss: 54.353736877441406\n",
            "Epoch 10 loss: 131.31301879882812\n",
            "Epoch 11 loss: 115.73565673828125\n",
            "Epoch 12 loss: 124.80242919921875\n",
            "Epoch 13 loss: 73.86136627197266\n",
            "Epoch 14 loss: 426.6602478027344\n",
            "Epoch 15 loss: 171.6125946044922\n",
            "Epoch 16 loss: 333.06085205078125\n",
            "Epoch 17 loss: 120.75927734375\n",
            "Epoch 18 loss: 143.71432495117188\n",
            "Epoch 19 loss: 170.67068481445312\n",
            "Epoch 20 loss: 28.470966339111328\n",
            "Epoch 21 loss: 75.09066009521484\n",
            "Epoch 22 loss: 214.44985961914062\n",
            "Epoch 23 loss: 159.35215759277344\n",
            "Epoch 24 loss: 210.3485565185547\n",
            "Epoch 25 loss: 99.95280456542969\n",
            "Epoch 26 loss: 141.62295532226562\n",
            "Epoch 27 loss: 55.7194938659668\n",
            "Epoch 28 loss: 273.6195983886719\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[103], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     21\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch, loss\u001b[39m.\u001b[39mitem()))\n\u001b[0;32m---> 23\u001b[0m train_model(model, optimizer, train_data_loader, loss_func, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Plotting the loss\u001b[39;00m\n\u001b[1;32m     26\u001b[0m plt\u001b[39m.\u001b[39mplot(losses)\n",
            "Cell \u001b[0;32mIn[103], line 5\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, data_loader, loss_module, num_epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 5\u001b[0m   \u001b[39mfor\u001b[39;00m data_point, data_label \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m      6\u001b[0m     data_point \u001b[39m=\u001b[39m data_point\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     data_label \u001b[39m=\u001b[39m data_label\u001b[39m.\u001b[39mto(device)\n",
            "File \u001b[0;32m~/Documents/Uni/artificial_intelligence/Homework/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/Documents/Uni/artificial_intelligence/Homework/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/Documents/Uni/artificial_intelligence/Homework/env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
            "File \u001b[0;32m~/Documents/Uni/artificial_intelligence/Homework/env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
            "File \u001b[0;32m~/Documents/Uni/artificial_intelligence/Homework/env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/Uni/artificial_intelligence/Homework/env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/Uni/artificial_intelligence/Homework/env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
            "File \u001b[0;32m~/Documents/Uni/artificial_intelligence/Homework/env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "losses = []\n",
        "def train_model(model, optimizer, data_loader, loss_module, num_epochs=1000):\n",
        "  model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "    for data_point, data_label in data_loader:\n",
        "      data_point = data_point.to(device)\n",
        "      data_label = data_label.to(device)\n",
        "      data_label = data_label.squeeze(dim=1)\n",
        "\n",
        "      output = model(data_point)\n",
        "      output = output.squeeze(dim=1)\n",
        "\n",
        "      # print(f\"Predicted: {output} - True value: {data_label}\")\n",
        "\n",
        "      loss = loss_module(output, data_label.float())\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "    print(\"Epoch {} loss: {}\".format(epoch, loss.item()))\n",
        "\n",
        "train_model(model, optimizer, train_data_loader, loss_func, num_epochs=500)\n",
        "\n",
        "# Plotting the loss\n",
        "plt.plot(losses)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
